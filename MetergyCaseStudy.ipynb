{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "611390de-7fd8-412a-a36a-e9b6524c0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp  measurement  uom attributes.Serial  \\\n",
      "0 2025-05-18 00:15:00+00:00     0.013578  2.0          70028794   \n",
      "1 2025-05-18 00:30:00+00:00     0.039125  2.0          70028794   \n",
      "2 2025-05-18 00:45:00+00:00     0.032656  2.0          70028794   \n",
      "3 2025-05-18 01:00:00+00:00     0.014164  2.0          70028794   \n",
      "4 2025-05-18 01:15:00+00:00     0.014125  2.0          70028794   \n",
      "\n",
      "  attributes.Phase attributes.Type attributes.TOU attributes.Unit  \\\n",
      "0               11       Intervals              0              kW   \n",
      "1               11       Intervals              0              kW   \n",
      "2               11       Intervals              0              kW   \n",
      "3               11       Intervals              0              kW   \n",
      "4               11       Intervals              0              kW   \n",
      "\n",
      "                  source_file  \n",
      "0  B-2224220025_20250518.json  \n",
      "1  B-2224220025_20250518.json  \n",
      "2  B-2224220025_20250518.json  \n",
      "3  B-2224220025_20250518.json  \n",
      "4  B-2224220025_20250518.json  \n",
      "2400\n",
      "Interval Records: (600, 9)\n",
      "                  timestamp  measurement  uom attributes.Serial  \\\n",
      "0 2025-05-18 01:00:00+00:00     0.034906    2          81015392   \n",
      "1 2025-05-18 02:00:00+00:00     0.034781    2          81015392   \n",
      "2 2025-05-18 03:00:00+00:00     0.037250    2          81015392   \n",
      "3 2025-05-18 04:00:00+00:00     0.036875    2          81015392   \n",
      "4 2025-05-18 05:00:00+00:00     0.035969    2          81015392   \n",
      "\n",
      "  attributes.Phase attributes.Type attributes.TOU attributes.Unit  \\\n",
      "0                7       Intervals              0              kW   \n",
      "1                7       Intervals              0              kW   \n",
      "2                7       Intervals              0              kW   \n",
      "3                7       Intervals              0              kW   \n",
      "4                7       Intervals              0              kW   \n",
      "\n",
      "                  source_file  \n",
      "0  B-2789678491_20250518.json  \n",
      "1  B-2789678491_20250518.json  \n",
      "2  B-2789678491_20250518.json  \n",
      "3  B-2789678491_20250518.json  \n",
      "4  B-2789678491_20250518.json  \n",
      "Register Records: (54, 9)\n",
      "                  timestamp   measurement  uom attributes.Serial  \\\n",
      "0 2025-05-18 00:00:00+00:00  18293.695921    3          81015392   \n",
      "1 2025-05-18 14:00:00+00:00      0.592000    2          81015392   \n",
      "2 2025-05-19 00:00:00+00:00  18296.105671    3          81015392   \n",
      "3 2025-05-19 19:00:00+00:00      0.133500    2          81015392   \n",
      "4 2025-05-20 00:00:00+00:00  18297.872192    3          81015392   \n",
      "\n",
      "  attributes.Phase attributes.Type attributes.TOU attributes.Unit  \\\n",
      "0                7     DailyAccums              1             kWH   \n",
      "1                7         Demands              1              kW   \n",
      "2                7     DailyAccums              1             kWH   \n",
      "3                7         Demands              1              kW   \n",
      "4                7     DailyAccums              1             kWH   \n",
      "\n",
      "                  source_file  \n",
      "0  B-2789678491_20250518.json  \n",
      "1  B-2789678491_20250518.json  \n",
      "2  B-2789678491_20250519.json  \n",
      "3  B-2789678491_20250519.json  \n",
      "4  B-2789678491_20250520.json  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Extracting everything in the folder B-2224220025\n",
    "folder_path = r\"C:\\Users\\dsai3\\Downloads\\B-2224220025 (1)\\B-2224220025\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Intitalize Dataframe\n",
    "all_dfs = []\n",
    "\n",
    "# Loop through each file and process it\n",
    "for file_name in json_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        df = pd.json_normalize(data['interval_records'])\n",
    "        df['source_file'] = file_name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "#Convert timestamp to datetime\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "\n",
    "# Display the result\n",
    "print(combined_df.head())\n",
    "print(len(combined_df))\n",
    "combined_df.to_csv(\"apartment_records.csv\", index=False, encoding='utf-8-sig')\n",
    "#print(f\"Excel File Created: {output_file}\")\n",
    "\n",
    "# Extracting everything in the folder B-2789678491\n",
    "folder_path = r\"C:\\Users\\dsai3\\Downloads\\B-2789678491 (1)\\B-2789678491\"\n",
    "\n",
    "# Lists to store records\n",
    "interval_dfs = []\n",
    "register_dfs = []\n",
    "\n",
    "# Loop through each JSON file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            if data.get(\"interval_records\"):\n",
    "                df_interval = pd.json_normalize(data['interval_records'])\n",
    "                df_interval['source_file'] = filename\n",
    "                interval_dfs.append(df_interval)\n",
    "            if data.get(\"register_records\"):\n",
    "                df_register = pd.json_normalize(data['register_records'])\n",
    "                df_register['source_file'] = filename\n",
    "                register_dfs.append(df_register)\n",
    "\n",
    "# Combine all data\n",
    "combined_interval_df = pd.concat(interval_dfs, ignore_index=True) if interval_dfs else pd.DataFrame()\n",
    "combined_register_df = pd.concat(register_dfs, ignore_index=True) if register_dfs else pd.DataFrame()\n",
    "\n",
    "# Optional: convert timestamp columns\n",
    "if not combined_interval_df.empty:\n",
    "    combined_interval_df['timestamp'] = pd.to_datetime(combined_interval_df['timestamp'])\n",
    "\n",
    "if not combined_register_df.empty:\n",
    "    combined_register_df['timestamp'] = pd.to_datetime(combined_register_df['timestamp'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Interval Records:\", combined_interval_df.shape)\n",
    "print(combined_interval_df.head())\n",
    "print(\"Register Records:\", combined_register_df.shape)\n",
    "print(combined_register_df.head())\n",
    "\n",
    "# Save to CSV if needed\n",
    "combined_interval_df.to_csv(\"combined_interval_records.csv\", index=False, encoding='utf-8-sig')\n",
    "combined_register_df.to_csv(\"combined_register_records.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11211465-7700-45a8-8e68-4060bc4a156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dates in the Dataset:\n",
      "2025-05-18\n",
      "2025-05-19\n",
      "2025-05-20\n",
      "2025-05-21\n",
      "2025-05-22\n",
      "2025-05-23\n",
      "2025-05-24\n",
      "2025-05-25\n",
      "2025-05-26\n",
      "2025-05-27\n",
      "2025-05-28\n",
      "2025-05-29\n",
      "2025-05-30\n",
      "2025-05-31\n",
      "2025-06-01\n",
      "2025-06-02\n",
      "2025-06-03\n",
      "2025-06-04\n",
      "2025-06-05\n",
      "2025-06-06\n",
      "2025-06-07\n",
      "2025-06-08\n",
      "2025-06-09\n",
      "2025-06-10\n",
      "2025-06-11\n",
      "2025-06-12\n",
      "2025-06-13\n",
      "timestamp            0\n",
      "measurement          0\n",
      "uom                  0\n",
      "attributes.Serial    0\n",
      "attributes.Phase     0\n",
      "attributes.Type      0\n",
      "attributes.TOU       0\n",
      "attributes.Unit      0\n",
      "source_file          0\n",
      "date                 0\n",
      "dtype: int64\n",
      "                     timestamp  measurement  uom attributes.Serial  \\\n",
      "208  2025-05-20 04:15:00+00:00      0.51850  2.0          70028794   \n",
      "209  2025-05-20 04:30:00+00:00      0.33200  2.0          70028794   \n",
      "282  2025-05-20 22:45:00+00:00      0.34225  2.0          70028794   \n",
      "283  2025-05-20 23:00:00+00:00      0.34775  2.0          70028794   \n",
      "284  2025-05-20 23:15:00+00:00      0.38175  2.0          70028794   \n",
      "...                        ...          ...  ...               ...   \n",
      "1961 2025-06-08 10:30:00+00:00      0.33650  2.0          70028794   \n",
      "1962 2025-06-08 10:45:00+00:00      0.41050  2.0          70028794   \n",
      "1993 2025-06-08 18:30:00+00:00      0.53600  2.0          70028794   \n",
      "2037 2025-06-09 05:30:00+00:00      0.45400  2.0          70028794   \n",
      "2223 2025-06-11 04:00:00+00:00      0.40425  2.0          70028794   \n",
      "\n",
      "     attributes.Phase attributes.Type attributes.TOU attributes.Unit  \\\n",
      "208                11       Intervals              0              kW   \n",
      "209                11       Intervals              0              kW   \n",
      "282                11       Intervals              0              kW   \n",
      "283                11       Intervals              0              kW   \n",
      "284                11       Intervals              0              kW   \n",
      "...               ...             ...            ...             ...   \n",
      "1961               11       Intervals              0              kW   \n",
      "1962               11       Intervals              0              kW   \n",
      "1993               11       Intervals              0              kW   \n",
      "2037               11       Intervals              0              kW   \n",
      "2223               11       Intervals              0              kW   \n",
      "\n",
      "                     source_file        date  \n",
      "208   B-2224220025_20250520.json  2025-05-20  \n",
      "209   B-2224220025_20250520.json  2025-05-20  \n",
      "282   B-2224220025_20250520.json  2025-05-20  \n",
      "283   B-2224220025_20250520.json  2025-05-20  \n",
      "284   B-2224220025_20250520.json  2025-05-20  \n",
      "...                          ...         ...  \n",
      "1961  B-2224220025_20250608.json  2025-06-08  \n",
      "1962  B-2224220025_20250608.json  2025-06-08  \n",
      "1993  B-2224220025_20250608.json  2025-06-08  \n",
      "2037  B-2224220025_20250609.json  2025-06-09  \n",
      "2223  B-2224220025_20250611.json  2025-06-11  \n",
      "\n",
      "[64 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#Looking at the unique timestamps\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], errors='coerce')\n",
    "combined_df['date'] = combined_df['timestamp'].dt.date\n",
    "unique_dates = combined_df['date'].dropna().unique()\n",
    "unique_dates = sorted(unique_dates)\n",
    "\n",
    "print(\"Unique Dates in the Dataset:\")\n",
    "for date in unique_dates:\n",
    "    print(date)\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "combined_df['measurement'].describe()\n",
    "combined_df[combined_df['measurement'] == 0]\n",
    "combined_df[combined_df['measurement'] < 0]\n",
    "combined_df[combined_df['measurement'] > combined_df['measurement'].quantile(0.999)]\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "z_scores = stats.zscore(combined_df['measurement'])\n",
    "outliers_z = combined_df[abs(z_scores) > 3]  # Anything > 3 std dev\n",
    "print(outliers_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a087f9b0-b6fd-40e9-b8b7-de62a04d6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dates in the Dataset:\n",
      "2025-05-18\n",
      "2025-05-19\n",
      "2025-05-20\n",
      "2025-05-21\n",
      "2025-05-22\n",
      "2025-05-23\n",
      "2025-05-24\n",
      "2025-05-25\n",
      "2025-05-26\n",
      "2025-05-27\n",
      "2025-05-28\n",
      "2025-05-29\n",
      "2025-05-30\n",
      "2025-05-31\n",
      "2025-06-01\n",
      "2025-06-02\n",
      "2025-06-03\n",
      "2025-06-04\n",
      "2025-06-05\n",
      "2025-06-06\n",
      "2025-06-07\n",
      "2025-06-08\n",
      "2025-06-09\n",
      "2025-06-10\n",
      "2025-06-11\n",
      "2025-06-12\n",
      "2025-06-13\n",
      "timestamp            0\n",
      "measurement          0\n",
      "uom                  0\n",
      "attributes.Serial    0\n",
      "attributes.Phase     0\n",
      "attributes.Type      0\n",
      "attributes.TOU       0\n",
      "attributes.Unit      0\n",
      "source_file          0\n",
      "date                 0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [timestamp, measurement, uom, attributes.Serial, attributes.Phase, attributes.Type, attributes.TOU, attributes.Unit, source_file]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Looking at the unique timestamps\n",
    "combined_interval_df['timestamp'] = pd.to_datetime(combined_interval_df['timestamp'], errors='coerce')\n",
    "combined_interval_df['date'] = combined_interval_df['timestamp'].dt.date\n",
    "unique_dates = combined_interval_df['date'].dropna().unique()\n",
    "unique_dates = sorted(unique_dates)\n",
    "\n",
    "print(\"Unique Dates in the Dataset:\")\n",
    "for date in unique_dates:\n",
    "    print(date)\n",
    "print(combined_interval_df.isnull().sum())\n",
    "\n",
    "z_scores = stats.zscore(combined_register_df['measurement'])\n",
    "outliers_z = combined_register_df[abs(z_scores) > 3]  # Anything > 3 std dev\n",
    "print(outliers_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceee0b63-99a8-46aa-b120-c65c612b0bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dates in the Dataset:\n",
      "2025-05-18\n",
      "2025-05-19\n",
      "2025-05-20\n",
      "2025-05-21\n",
      "2025-05-22\n",
      "2025-05-23\n",
      "2025-05-24\n",
      "2025-05-25\n",
      "2025-05-26\n",
      "2025-05-27\n",
      "2025-05-28\n",
      "2025-05-29\n",
      "2025-05-30\n",
      "2025-05-31\n",
      "2025-06-01\n",
      "2025-06-02\n",
      "2025-06-03\n",
      "2025-06-04\n",
      "2025-06-05\n",
      "2025-06-06\n",
      "2025-06-08\n",
      "2025-06-09\n",
      "2025-06-10\n",
      "2025-06-11\n",
      "2025-06-12\n",
      "2025-06-13\n",
      "2025-06-14\n",
      "2025-06-15\n",
      "2025-06-16\n",
      "timestamp            0\n",
      "measurement          0\n",
      "uom                  0\n",
      "attributes.Serial    0\n",
      "attributes.Phase     0\n",
      "attributes.Type      0\n",
      "attributes.TOU       0\n",
      "attributes.Unit      0\n",
      "source_file          0\n",
      "date                 0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [timestamp, measurement, uom, attributes.Serial, attributes.Phase, attributes.Type, attributes.TOU, attributes.Unit, source_file, date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Looking at the unique timestamps\n",
    "combined_register_df['timestamp'] = pd.to_datetime(combined_register_df['timestamp'], errors='coerce')\n",
    "combined_register_df['date'] = combined_register_df['timestamp'].dt.date\n",
    "unique_dates = combined_register_df['date'].dropna().unique()\n",
    "unique_dates = sorted(unique_dates)\n",
    "\n",
    "print(\"Unique Dates in the Dataset:\")\n",
    "for date in unique_dates:\n",
    "    print(date)\n",
    "print(combined_register_df.isnull().sum())\n",
    "\n",
    "z_scores = stats.zscore(combined_register_df['measurement'])\n",
    "outliers_z = combined_register_df[abs(z_scores) > 3]  # Anything > 3 std dev\n",
    "print(outliers_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bcee45a-80d5-4c93-a760-2ef59ac388ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.duplicated().sum())\n",
    "print(combined_register_df.duplicated().sum())\n",
    "print(combined_interval_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f7e797a-2828-4afb-bd36-2845634cc58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   attributes.Serial        date  missing_count  \\\n",
      "0           70028794  2025-05-18            1.0   \n",
      "20          70028794  2025-06-07           95.0   \n",
      "21          70028794  2025-06-08            1.0   \n",
      "26          70028794  2025-06-13           95.0   \n",
      "\n",
      "                                        missing_times  \n",
      "0                                          [00:00:00]  \n",
      "20  [00:15:00, 00:30:00, 00:45:00, 01:00:00, 01:15...  \n",
      "21                                         [00:00:00]  \n",
      "26  [00:15:00, 00:30:00, 00:45:00, 01:00:00, 01:15...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsai3\\AppData\\Local\\Temp\\ipykernel_39908\\3872775335.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  missing_summary = combined_df.groupby(['attributes.Serial', 'date']).apply(find_missing_intervals).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Columns needed: 'timestamp', 'apartment_id' (e.g., Serial), etc.\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "\n",
    "# Extract date and floor timestamp to nearest 15 minutes\n",
    "combined_df['date'] = combined_df['timestamp'].dt.date\n",
    "combined_df['rounded_time'] = combined_df['timestamp'].dt.floor('15min')\n",
    "\n",
    "# Create expected 15-min interval index for one full day\n",
    "start_time = pd.Timestamp(\"00:00:00\")\n",
    "end_time = pd.Timestamp(\"23:45:00\")\n",
    "expected_times = pd.date_range(start=start_time, end=end_time, freq='15min').time\n",
    "expected_count = len(expected_times)  # Should be 96 for 15-min intervals\n",
    "\n",
    "# Function to check missing timestamps for one group\n",
    "def find_missing_intervals(group):\n",
    "    actual_times = group['rounded_time'].dt.time\n",
    "    missing = set(expected_times) - set(actual_times)\n",
    "    return pd.Series({\n",
    "        'missing_count': len(missing),\n",
    "        'missing_times': sorted(list(missing)) if missing else None\n",
    "    })\n",
    "\n",
    "# Apply group check\n",
    "missing_summary = combined_df.groupby(['attributes.Serial', 'date']).apply(find_missing_intervals).reset_index()\n",
    "\n",
    "# Filter for groups with missing timestamps\n",
    "missing_summary = missing_summary[missing_summary['missing_count'] > 0]\n",
    "\n",
    "print(missing_summary)\n",
    "missing_summary.to_csv(\"missing_summary.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e364b-c128-4b6a-9dc2-0720e3a47380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2829817-98a6-4dc2-b993-c0b4c04fca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461ae1-f4c6-4099-bae2-7ec2ffa24edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f4abf-f5d8-49d4-8c13-4cae94d53662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
